{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPZ3/5bOf5PRSWiBbTjRdCa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CeorgeCao-HG/Reinforcement_Learning-RL/blob/main/RL_Vanilla_Policy_Gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vSqqToyTVE4N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d778aebf-720d-46ff-df6b-de2381ce39af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m129.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.23.5\n",
        "!pip install gym\n",
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym.spaces import Discrete, Box\n",
        "\n",
        "def mlp(sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
        "    # Build a feedforward neural network.\n",
        "    layers = []\n",
        "    for j in range(len(sizes)-1):\n",
        "        act = activation if j < len(sizes)-2 else output_activation\n",
        "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2,\n",
        "          epochs=50, batch_size=5000, render=False):\n",
        "\n",
        "    # make environment, check spaces, get obs / act dims\n",
        "    env = gym.make(env_name)\n",
        "    assert isinstance(env.observation_space, Box), \\\n",
        "        \"This example only works for envs with continuous state spaces.\"\n",
        "    assert isinstance(env.action_space, Discrete), \\\n",
        "        \"This example only works for envs with discrete action spaces.\"\n",
        "\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    n_acts = env.action_space.n\n",
        "\n",
        "    # make core of policy network\n",
        "    logits_net = mlp(sizes=[obs_dim]+hidden_sizes+[n_acts])\n",
        "\n",
        "    # make function to compute action distribution\n",
        "    def get_policy(obs):\n",
        "        logits = logits_net(obs)\n",
        "        return Categorical(logits=logits)\n",
        "\n",
        "    # make action selection function (outputs int actions, sampled from policy)\n",
        "    def get_action(obs):\n",
        "        return get_policy(obs).sample().item()\n",
        "\n",
        "    # make loss function whose gradient, for the right data, is policy gradient\n",
        "    def compute_loss(obs, act, weights):\n",
        "        logp = get_policy(obs).log_prob(act)\n",
        "        return -(logp * weights).mean()\n",
        "\n",
        "    # make optimizer\n",
        "    optimizer = Adam(logits_net.parameters(), lr=lr)\n",
        "\n",
        "    # for training policy\n",
        "    def train_one_epoch():\n",
        "        # make some empty lists for logging.\n",
        "        batch_obs = []          # for observations\n",
        "        batch_acts = []         # for actions\n",
        "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
        "        batch_rets = []         # for measuring episode returns\n",
        "        batch_lens = []         # for measuring episode lengths\n",
        "\n",
        "        # reset episode-specific variables\n",
        "        obs = env.reset()       # first obs comes from starting distribution\n",
        "        done = False            # signal from environment that episode is over\n",
        "        ep_rews = []            # list for rewards accrued throughout ep\n",
        "\n",
        "        # render first episode of each epoch\n",
        "        finished_rendering_this_epoch = False\n",
        "\n",
        "        # collect experience by acting in the environment with current policy\n",
        "        while True:\n",
        "\n",
        "            # rendering\n",
        "            if (not finished_rendering_this_epoch) and render:\n",
        "                env.render()\n",
        "\n",
        "            # save obs\n",
        "            batch_obs.append(obs.copy())\n",
        "\n",
        "            # act in the environment\n",
        "            act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
        "            obs, rew, done, _ = env.step(act)\n",
        "\n",
        "            # save action, reward\n",
        "            batch_acts.append(act)\n",
        "            ep_rews.append(rew)\n",
        "\n",
        "            if done:\n",
        "                # if episode is over, record info about episode\n",
        "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
        "                batch_rets.append(ep_ret)\n",
        "                batch_lens.append(ep_len)\n",
        "\n",
        "                # the weight for each logprob(a|s) is R(tau)\n",
        "                batch_weights += [ep_ret] * ep_len\n",
        "\n",
        "                # reset episode-specific variables\n",
        "                obs, done, ep_rews = env.reset(), False, []\n",
        "\n",
        "                # won't render again this epoch\n",
        "                finished_rendering_this_epoch = True\n",
        "\n",
        "                # end experience loop if we have enough of it\n",
        "                if len(batch_obs) > batch_size:\n",
        "                    break\n",
        "\n",
        "        # take a single policy gradient update step\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
        "                                  act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
        "                                  weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
        "                                  )\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "        return batch_loss, batch_rets, batch_lens\n",
        "\n",
        "    # training loop\n",
        "    for i in range(epochs):\n",
        "        batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
        "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
        "                (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "    import sys\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--env_name', '--env', type=str, default='CartPole-v0')\n",
        "    parser.add_argument('--render', action='store_true')\n",
        "    parser.add_argument('--lr', type=float, default=1e-2)\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    # Option 1: Conditional parsing\n",
        "    # Check if running in Colab\n",
        "    if 'google.colab' in sys.modules:\n",
        "      args = parser.parse_args([])  # Use defaults in Colab\n",
        "    else:\n",
        "      args = parser.parse_args()    # Normal command-line parsing\n",
        "\n",
        "    # Option 2: Use parse_known_args (handles unknown arguments)\n",
        "    args, unknown = parser.parse_known_args()  # Ignores unknown args\n",
        "\n",
        "\n",
        "    print('\\nUsing simplest formulation of policy gradient.\\n')\n",
        "    train(env_name=args.env_name, render=args.render, lr=args.lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0bv-ZkagYAy",
        "outputId": "29924960-0b41-4827-b802-774962d25519"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using simplest formulation of policy gradient.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:   0 \t loss: 22.424 \t return: 25.125 \t ep_len: 25.125\n",
            "epoch:   1 \t loss: 25.385 \t return: 26.749 \t ep_len: 26.749\n",
            "epoch:   2 \t loss: 26.340 \t return: 28.908 \t ep_len: 28.908\n",
            "epoch:   3 \t loss: 30.283 \t return: 32.290 \t ep_len: 32.290\n",
            "epoch:   4 \t loss: 29.717 \t return: 35.098 \t ep_len: 35.098\n",
            "epoch:   5 \t loss: 39.237 \t return: 43.129 \t ep_len: 43.129\n",
            "epoch:   6 \t loss: 36.371 \t return: 44.381 \t ep_len: 44.381\n",
            "epoch:   7 \t loss: 44.667 \t return: 49.693 \t ep_len: 49.693\n",
            "epoch:   8 \t loss: 42.041 \t return: 48.612 \t ep_len: 48.612\n",
            "epoch:   9 \t loss: 41.577 \t return: 49.911 \t ep_len: 49.911\n",
            "epoch:  10 \t loss: 39.115 \t return: 47.262 \t ep_len: 47.262\n",
            "epoch:  11 \t loss: 49.677 \t return: 62.025 \t ep_len: 62.025\n",
            "epoch:  12 \t loss: 45.958 \t return: 55.117 \t ep_len: 55.117\n",
            "epoch:  13 \t loss: 54.316 \t return: 70.722 \t ep_len: 70.722\n",
            "epoch:  14 \t loss: 55.276 \t return: 75.493 \t ep_len: 75.493\n",
            "epoch:  15 \t loss: 64.316 \t return: 84.949 \t ep_len: 84.949\n",
            "epoch:  16 \t loss: 58.207 \t return: 80.429 \t ep_len: 80.429\n",
            "epoch:  17 \t loss: 70.907 \t return: 95.185 \t ep_len: 95.185\n",
            "epoch:  18 \t loss: 68.888 \t return: 103.245 \t ep_len: 103.245\n",
            "epoch:  19 \t loss: 68.817 \t return: 96.904 \t ep_len: 96.904\n",
            "epoch:  20 \t loss: 79.831 \t return: 117.295 \t ep_len: 117.295\n",
            "epoch:  21 \t loss: 89.313 \t return: 137.514 \t ep_len: 137.514\n",
            "epoch:  22 \t loss: 88.191 \t return: 140.405 \t ep_len: 140.405\n",
            "epoch:  23 \t loss: 89.783 \t return: 142.306 \t ep_len: 142.306\n",
            "epoch:  24 \t loss: 98.082 \t return: 161.581 \t ep_len: 161.581\n",
            "epoch:  25 \t loss: 99.685 \t return: 163.097 \t ep_len: 163.097\n",
            "epoch:  26 \t loss: 98.173 \t return: 162.375 \t ep_len: 162.375\n",
            "epoch:  27 \t loss: 103.747 \t return: 179.207 \t ep_len: 179.207\n",
            "epoch:  28 \t loss: 99.618 \t return: 172.300 \t ep_len: 172.300\n",
            "epoch:  29 \t loss: 102.589 \t return: 179.036 \t ep_len: 179.036\n",
            "epoch:  30 \t loss: 99.970 \t return: 176.034 \t ep_len: 176.034\n",
            "epoch:  31 \t loss: 105.687 \t return: 184.143 \t ep_len: 184.143\n",
            "epoch:  32 \t loss: 107.134 \t return: 191.519 \t ep_len: 191.519\n",
            "epoch:  33 \t loss: 103.825 \t return: 183.964 \t ep_len: 183.964\n",
            "epoch:  34 \t loss: 105.840 \t return: 190.370 \t ep_len: 190.370\n",
            "epoch:  35 \t loss: 105.433 \t return: 187.407 \t ep_len: 187.407\n",
            "epoch:  36 \t loss: 107.430 \t return: 193.231 \t ep_len: 193.231\n",
            "epoch:  37 \t loss: 106.336 \t return: 192.148 \t ep_len: 192.148\n",
            "epoch:  38 \t loss: 109.039 \t return: 199.731 \t ep_len: 199.731\n",
            "epoch:  39 \t loss: 108.494 \t return: 197.385 \t ep_len: 197.385\n",
            "epoch:  40 \t loss: 107.447 \t return: 198.423 \t ep_len: 198.423\n",
            "epoch:  41 \t loss: 107.857 \t return: 192.500 \t ep_len: 192.500\n",
            "epoch:  42 \t loss: 107.659 \t return: 196.654 \t ep_len: 196.654\n",
            "epoch:  43 \t loss: 107.367 \t return: 197.077 \t ep_len: 197.077\n",
            "epoch:  44 \t loss: 102.615 \t return: 184.964 \t ep_len: 184.964\n",
            "epoch:  45 \t loss: 102.436 \t return: 184.214 \t ep_len: 184.214\n",
            "epoch:  46 \t loss: 104.731 \t return: 187.370 \t ep_len: 187.370\n",
            "epoch:  47 \t loss: 104.324 \t return: 187.741 \t ep_len: 187.741\n",
            "epoch:  48 \t loss: 105.719 \t return: 192.654 \t ep_len: 192.654\n",
            "epoch:  49 \t loss: 103.673 \t return: 185.179 \t ep_len: 185.179\n"
          ]
        }
      ]
    }
  ]
}